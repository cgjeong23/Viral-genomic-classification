{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLT-kTPRw2a7",
        "outputId": "6e7c9e1a-174e-4035-f5fb-4831ac93c306"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.7/dist-packages (0.12.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsAs362r7Gzq",
        "outputId": "6df76396-0179-4b52-eb06-f18a2a7e31c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/cgjeong23/Viral-genomic-classification.git"
      ],
      "metadata": {
        "id": "-giXsqCTNVq-",
        "outputId": "806545e2-d6c7-4698-ae07-1a8d5ad3825e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Viral-genomic-classification'...\n",
            "remote: Enumerating objects: 50, done.\u001b[K\n",
            "remote: Counting objects: 100% (50/50), done.\u001b[K\n",
            "remote: Compressing objects: 100% (34/34), done.\u001b[K\n",
            "remote: Total 50 (delta 17), reused 47 (delta 14), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (50/50), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv Viral-genomic-classification virus"
      ],
      "metadata": {
        "id": "mbCr-B0TOGa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0ziw01lNU9t"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "lHVuLYHONU9u"
      },
      "outputs": [],
      "source": [
        "from virus.ML.model import RnnModel, RnnModelForClassification\n",
        "from virus.ML.train import train, evaluate\n",
        "from virus.ML.dataloader import load_sequences, sample_data, get_3_splits, SequenceDataset\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrCrHVRv4Nuz"
      },
      "source": [
        "# Do Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndB83Lk_8-du"
      },
      "outputs": [],
      "source": [
        "base_path = '/content/drive/MyDrive/trainingdata'\n",
        "\n",
        "tokenizer_file = '/content/drive/MyDrive/gene_tokenizer.json'\n",
        "\n",
        "sequences, labels = load_sequences(base_path)\n",
        "\n",
        "(train_seq, valid_seq, test_seq,\n",
        " train_label, valid_label, test_label) = get_3_splits(sequences, labels)\n",
        "\n",
        "train_dataset = SequenceDataset(train_seq, train_label, tokenizer_file=tokenizer_file)\n",
        "valid_dataset = SequenceDataset(valid_seq, valid_label, tokenizer_file=tokenizer_file,\n",
        "                                label_dict=train_dataset.label_dict)\n",
        "test_dataset = SequenceDataset(test_seq, test_label, tokenizer_file=tokenizer_file,\n",
        "                               label_dict=train_dataset.label_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwT_tWpX5czF"
      },
      "outputs": [],
      "source": [
        "lr = 1e-4\n",
        "batch_size = 5000\n",
        "num_epochs = 1\n",
        "vocab_size = train_dataset.tokenizer.get_vocab_size()\n",
        "pad_id = train_dataset.tokenizer.padding['pad_id']\n",
        "embedding_dim = 256\n",
        "hidden_dim = 512\n",
        "num_layers = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ilx5kkj74d1h"
      },
      "outputs": [],
      "source": [
        "#model = RnnModel(vocab_size, embedding_dim, pad_id, hidden_dim, num_layers)\n",
        "model = RnnModelForClassification(vocab_size, embedding_dim, pad_id, hidden_dim, num_layers, len(train_dataset.label_dict))\n",
        "model = model.to('cuda')\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=train_dataset.collate_fn)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, collate_fn=valid_dataset.collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=test_dataset.collate_fn)\n",
        "loss_function = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IV9nHKxT4PPy"
      },
      "outputs": [],
      "source": [
        "acc_history = train(model, train_loader, loss_function, lr, num_epochs, \n",
        "                    valid_loader=valid_loader, test_loader=test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vx05lI43xmrb"
      },
      "outputs": [],
      "source": [
        "acc_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Or05uIYlySRY"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(acc_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eY2KcLoHye2c"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "RNN Training.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "interpreter": {
      "hash": "1242238a27090cfd0a9f95cca18072a5b0343382fab30269241b63569e4c53cd"
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 64-bit ('datascience': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "e79170ae290057d802c0bc8323610d82df653b25ece5c46593757efbea4a04e3"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}