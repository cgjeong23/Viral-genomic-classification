{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"pLT-kTPRw2a7","outputId":"6e7c9e1a-174e-4035-f5fb-4831ac93c306"},"outputs":[],"source":["!pip install tokenizers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OsAs362r7Gzq","outputId":"6df76396-0179-4b52-eb06-f18a2a7e31c8"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-giXsqCTNVq-","outputId":"806545e2-d6c7-4698-ae07-1a8d5ad3825e"},"outputs":[],"source":["!git clone https://github.com/cgjeong23/Viral-genomic-classification.git virus"]},{"cell_type":"markdown","metadata":{"id":"L0ziw01lNU9t"},"source":["## Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lHVuLYHONU9u"},"outputs":[],"source":["from virus.ML.model import RnnModel, RnnModelForClassification\n","from virus.ML.train import train, evaluate\n","from virus.ML.dataloader import load_sequences, sample_data, get_3_splits, SequenceDataset\n","from torch import nn\n","from torch.utils.data import DataLoader\n","\n","import numpy as np\n","import os\n","\n","%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"markdown","metadata":{"id":"XrCrHVRv4Nuz"},"source":["# Do Training"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["use_google_drive = False\n","use_kaggle = True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ndB83Lk_8-du"},"outputs":[],"source":["google_drive_path = '/content/drive/MyDrive'\n","\n","if use_google_drive:\n","    base_path = f'{google_drive_path}/trainingdata'\n","    tokenizer_file = f'{google_drive_path}/gene_tokenizer.json' \n","elif use_kaggle:\n","    base_path = '../input/pacific-sra/trainingdata'\n","    tokenizer_file = '../input/pacific-sra/gene_tokenizer.json'\n","else:\n","    base_path = 'trainingdata'\n","    tokenizer_file = 'gene_tokenizer.json'\n","\n","sequences, labels = load_sequences(base_path)\n","\n","(train_seq, valid_seq, test_seq,\n"," train_label, valid_label, test_label) = get_3_splits(sequences, labels)\n","\n","label_dict = {k: i for i, k in enumerate(np.unique(labels))}\n","\n","train_dataset = SequenceDataset(train_seq, train_label, tokenizer_file=tokenizer_file,\n","                                label_dict=label_dict)\n","valid_dataset = SequenceDataset(valid_seq, valid_label, tokenizer_file=tokenizer_file,\n","                                label_dict=label_dict)\n","test_dataset = SequenceDataset(test_seq, test_label, tokenizer_file=tokenizer_file,\n","                               label_dict=label_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.utils.class_weight import compute_class_weight\n","import numpy as np\n","\n","class_weight = compute_class_weight('balanced', classes=np.unique(train_label), y=train_label)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from collections import Counter\n","\n","Counter(labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CwT_tWpX5czF"},"outputs":[],"source":["lr = 1e-2\n","batch_size = 5000\n","num_epochs = 1\n","vocab_size = train_dataset.tokenizer.get_vocab_size()\n","pad_id = train_dataset.tokenizer.padding['pad_id']\n","embedding_dim = 256\n","hidden_dim = 512\n","num_layers = 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ilx5kkj74d1h"},"outputs":[],"source":["#model = RnnModel(vocab_size, embedding_dim, pad_id, hidden_dim, num_layers)\n","import torch\n","model = RnnModelForClassification(vocab_size, embedding_dim, pad_id, hidden_dim, num_layers, len(train_dataset.label_dict))\n","model = model.to('cuda')\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=train_dataset.collate_fn)\n","valid_loader = DataLoader(valid_dataset, batch_size=batch_size, collate_fn=valid_dataset.collate_fn)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=test_dataset.collate_fn)\n","loss_function = nn.CrossEntropyLoss(weight=torch.Tensor(class_weight).to(\"cuda\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IV9nHKxT4PPy"},"outputs":[],"source":["kaggle_path = '/kaggle/working'\n","\n","if use_kaggle:\n","    save_path = kaggle_path\n","elif use_google_drive:\n","    save_path = google_drive_path\n","else:\n","    save_path = '.'\n","\n","acc_history = train(model, train_loader, loss_function, lr, num_epochs, \n","                    valid_loader=valid_loader, test_loader=test_loader,\n","                    base_path=save_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vx05lI43xmrb"},"outputs":[],"source":["acc_history"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Or05uIYlySRY"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","plt.plot(acc_history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eY2KcLoHye2c"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}