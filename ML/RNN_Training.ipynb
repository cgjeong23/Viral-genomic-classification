{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLT-kTPRw2a7",
        "outputId": "409c57ff-7c46-4eb4-a70e-7034ba679fc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.7/dist-packages (0.12.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsAs362r7Gzq",
        "outputId": "d3802292-cd8b-47a9-b6a4-1ad0270ffbcc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sZWclAPr9iv"
      },
      "source": [
        "## DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ftlQvoIZ7D6w"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "\n",
        "from tokenizers import Tokenizer, models\n",
        "\n",
        "base_path = '/content/drive/MyDrive/trainingdata'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "01_mwR757D6w"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def load_sequences(base_path):\n",
        "    sequences = []\n",
        "    labels = []\n",
        "    for folder in os.listdir(base_path):\n",
        "        if os.path.isdir(f'{base_path}/{folder}'):\n",
        "            for file_name in os.listdir(f'{base_path}/{folder}'):\n",
        "                fname = f'{base_path}/{folder}/{file_name}'\n",
        "                with open(fname) as f:\n",
        "                    for line in f:\n",
        "                        if line.startswith('>'):\n",
        "                            continue\n",
        "                        sequences.append(line)\n",
        "                        labels.append(folder)\n",
        "    return sequences, labels\n",
        "\n",
        "def sample_data(sequences, labels):\n",
        "    idx = np.random.choice(len(labels), int(len(labels) * 0.5))\n",
        "\n",
        "    seqeunces = [sequences[i] for i in idx]\n",
        "    labels = [labels[i] for i in idx]\n",
        "\n",
        "    return sequences, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def get_3_splits(sequences, labels):\n",
        "    (train_sequence, \n",
        "     test_sequence, \n",
        "     train_label, \n",
        "     test_label) = train_test_split(sequences, labels, test_size=0.2)\n",
        "\n",
        "    (valid_sequence, \n",
        "     test_sequence, \n",
        "     valid_label,\n",
        "     test_label) = train_test_split(test_sequence, test_label, test_size=0.5)\n",
        "\n",
        "    return (train_sequence, valid_sequence, test_sequence, \n",
        "            train_label, valid_label, test_label)\n"
      ],
      "metadata": {
        "id": "ZKN035vbBNS2"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "jpEC676_rdlz"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "class SequenceDataset(Dataset):\n",
        "    def __init__(self, sequence, labels, tokenizer_file='gene_tokenizer.json',\n",
        "                 label_dict=None):\n",
        "        \"\"\"sequence: List of Str\n",
        "        \n",
        "        [\"ACTG...\", \"GTCA...\", ...]\n",
        "        \"\"\"\n",
        "        self.sequence = sequence\n",
        "        if label_dict is None:\n",
        "            self.label_dict = self.get_label_dict(labels)\n",
        "        else:\n",
        "            self.label_dict = label_dict\n",
        "        self.labels = self.encode_labels(labels)\n",
        "\n",
        "        self.tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n",
        "        self.tokenizer = self.tokenizer.from_file(tokenizer_file)\n",
        "        self.tokenizer.enable_padding()\n",
        "\n",
        "    def get_label_dict(self, labels):\n",
        "        label_set = set(labels)\n",
        "        label_dict = {}\n",
        "        for i, x in enumerate(label_set):\n",
        "            label_dict[x] = i\n",
        "        \n",
        "        return label_dict\n",
        "    \n",
        "    def encode_labels(self, labels):\n",
        "        encoded_label = []  \n",
        "        for y in labels:\n",
        "            encoded_label.append(self.label_dict[y])\n",
        "\n",
        "        return encoded_label\n",
        "            \n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.sequence)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq = self.sequence[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoded_seq = self.tokenizer.encode(seq)\n",
        "        return torch.LongTensor(encoded_seq.ids), label\n",
        "\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        \"\"\"batch: list of (torch.LongTensor, int)\"\"\"\n",
        "        sequences = []\n",
        "        labels = []\n",
        "        for item in batch:\n",
        "            sequences.append(item[0])\n",
        "            labels.append(item[1])\n",
        "            \n",
        "        sequence = pad_sequence(sequences, batch_first=True, padding_value=self.tokenizer.padding['pad_id'])\n",
        "        labels = torch.LongTensor(labels)\n",
        "        \n",
        "        return sequence, labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMS3-rETr_Mt"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "nTejCh9xr_1n"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "class RnnModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, pad_id, hidden_dim, num_layers):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_id)\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.out_layer = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, ids):\n",
        "        # ids: [batch size, max sequence length] = [B, L]\n",
        "        embedded = self.embedding(ids)  # [B, L, E]\n",
        "        rnn_out, _ = self.rnn(embedded)  # [B, L, H]\n",
        "        return self.out_layer(rnn_out)  # [B, L, V]\n",
        "\n",
        "class RnnModelForClassification(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, pad_id, hidden_dim, num_layers, output_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_id)\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.out_layer = nn.Linear(hidden_dim, output_size)\n",
        "    \n",
        "    def forward(self, ids):\n",
        "        # ids: [batch size, max sequence length] = [B, L]\n",
        "        embedded = self.embedding(ids)  # [B, L, E]\n",
        "        rnn_out, _ = self.rnn(embedded)  # [B, L, H]\n",
        "        hidden_state = rnn_out[:, -1, :]  # [B, H]\n",
        "        return self.out_layer(hidden_state)  # [B, C]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgkBpBRNsBXh"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Umon_VN4sB-q"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "def train(model, dataloader, loss_function, lr, num_epochs, valid_loader=None,\n",
        "          test_loader=None):\n",
        "    os.makedirs('/content/drive/MyDrive/GeneModels', exist_ok=True)\n",
        "    # pytorch training loop\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    history = {'train': [], 'valid': [], 'test': []}\n",
        "    for epoch in range(num_epochs):\n",
        "        pbar = tqdm(dataloader)\n",
        "\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        for batch in pbar:\n",
        "\n",
        "            batch_sequences, y = batch\n",
        "            x = batch_sequences.to('cuda')\n",
        "            y = y.to('cuda')\n",
        "\n",
        "            h = model(x)  # [B, C]\n",
        "            j = loss_function(h, y)\n",
        "            \n",
        "            # do gradient descent\n",
        "            optimizer.zero_grad()  # remove junk from last step\n",
        "            j.backward()   # calculate gradient from current batch outputs\n",
        "            optimizer.step()  # update the weights using the gradients\n",
        "\n",
        "            all_preds.append(h.argmax(-1).detach().cpu())\n",
        "            all_labels.append(y.cpu())\n",
        "\n",
        "        all_preds = torch.cat(all_preds).numpy()\n",
        "        all_labels = torch.cat(all_labels).numpy()\n",
        "    \n",
        "        print(classification_report(all_labels, all_preds, digits=4))\n",
        "        accuracy = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "        if valid_loader is not None:\n",
        "            val_accuracy = evaluate(valid_loader)\n",
        "            history['valid'].append(val_accuracy)\n",
        "        if test_loader is not None:\n",
        "            test_accuracy = evaluate(test_loader)\n",
        "            history['test'].append(test_accuracy)\n",
        "\n",
        "        history['train'].append(accuracy)\n",
        "\n",
        "        torch.save(model.state_dict(), f'/content/drive/MyDrive/GeneModels/{epoch}.pth')\n",
        "        \n",
        "        \n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "def evaluate(valid_loader):\n",
        "    valid_preds = []\n",
        "    valid_labels = []\n",
        "    for batch in valid_loader:\n",
        "        batch_sequences, y = batch\n",
        "        x = batch_sequences.to('cuda')\n",
        "        y = y.to('cuda')\n",
        "        h = model(x)  # [B, C]\n",
        "        valid_preds.append(h.argmax(-1).detach().cpu())\n",
        "        valid_labels.append(y.cpu())\n",
        "    valid_preds = torch.cat(valid_preds).numpy()\n",
        "    valid_labels = torch.cat(valid_labels).numpy()\n",
        "\n",
        "    print(classification_report(valid_labels, valid_preds, digits=4))\n",
        "    accuracy = accuracy_score(valid_labels, valid_preds)\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrCrHVRv4Nuz"
      },
      "source": [
        "# Do Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_file = '/content/drive/MyDrive/gene_tokenizer.json'\n",
        "\n",
        "sequences, labels = load_sequences(base_path)\n",
        "\n",
        "(train_seq, valid_seq, test_seq,\n",
        " train_label, valid_label, test_label) = get_3_splits(sequences, labels)\n",
        "\n",
        "train_dataset = SequenceDataset(train_seq, train_label, tokenizer_file=tokenizer_file)\n",
        "valid_dataset = SequenceDataset(valid_seq, valid_label, tokenizer_file=tokenizer_file,\n",
        "                                label_dict=train_dataset.label_dict)\n",
        "test_dataset = SequenceDataset(test_seq, test_label, tokenizer_file=tokenizer_file,\n",
        "                               label_dict=train_dataset.label_dict)"
      ],
      "metadata": {
        "id": "ndB83Lk_8-du"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "CwT_tWpX5czF"
      },
      "outputs": [],
      "source": [
        "lr = 1e-4\n",
        "batch_size = 5000\n",
        "num_epochs = 1\n",
        "vocab_size = train_dataset.tokenizer.get_vocab_size()\n",
        "pad_id = train_dataset.tokenizer.padding['pad_id']\n",
        "embedding_dim = 256\n",
        "hidden_dim = 512\n",
        "num_layers = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "ilx5kkj74d1h"
      },
      "outputs": [],
      "source": [
        "#model = RnnModel(vocab_size, embedding_dim, pad_id, hidden_dim, num_layers)\n",
        "model = RnnModelForClassification(vocab_size, embedding_dim, pad_id, hidden_dim, num_layers, len(train_dataset.label_dict))\n",
        "model = model.to('cuda')\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=train_dataset.collate_fn)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, collate_fn=valid_dataset.collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=test_dataset.collate_fn)\n",
        "loss_function = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IV9nHKxT4PPy"
      },
      "outputs": [],
      "source": [
        "acc_history = train(model, train_loader, loss_function, lr, num_epochs, \n",
        "                    valid_loader=valid_loader, test_loader=test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vx05lI43xmrb"
      },
      "outputs": [],
      "source": [
        "acc_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Or05uIYlySRY"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(acc_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eY2KcLoHye2c"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "RNN Training.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "interpreter": {
      "hash": "1242238a27090cfd0a9f95cca18072a5b0343382fab30269241b63569e4c53cd"
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 64-bit ('datascience': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "e79170ae290057d802c0bc8323610d82df653b25ece5c46593757efbea4a04e3"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}